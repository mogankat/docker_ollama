services:
  nginx-lb:
    image: nginx:latest
    container_name: nginx-lb
    ports:
      - "8081:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - mistral
      - llama3
      - gemma3
      - deepseek
      - qwen3
      - qwen3-coder

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    environment:
      - OLLAMA_BASE_URLS=http://nginx-lb/node1/;http://nginx-lb/node2/;http://nginx-lb/node3/;http://nginx-lb/node4/;http://nginx-lb/node5/;http://nginx-lb/node6/
      - ENABLE_PERSISTENT_CONFIG=False
      - ENABLE_AUTO_FUNCTIONS=True
      - FUNCTIONS_DIR=/app/backend/functions
      - ENABLE_FUNCTIONS_DIR_SYNC=True
      - WEBUI_NAME=I, for One, Welcome Our New AI Overlords
      - CORS_ALLOW_ORIGIN=*
    volumes:
      - ./open-webui:/app/backend/data
      - ./open-webui-data/functions:/app/backend/functions
    restart: always

  mistral:
    image: ollama/ollama:latest
    container_name: ollama-mistral
    volumes: ["./ollama_models/mistral:/root/.ollama"]
    restart: unless-stopped

  llama3:
    image: ollama/ollama:latest
    container_name: ollama-llama3
    volumes: ["./ollama_models/llama3:/root/.ollama"]
    restart: unless-stopped

  gemma3:
    image: ollama/ollama:latest
    container_name: ollama-gemma3
    volumes: ["./ollama_models/gemma3:/root/.ollama"]
    restart: unless-stopped

  deepseek:
    image: ollama/ollama:latest
    container_name: ollama-deepseek
    volumes: ["./ollama_models/deepseek:/root/.ollama"]
    restart: unless-stopped

  qwen3:
    image: ollama/ollama:latest
    container_name: ollama-qwen3
    volumes: ["./ollama_models/quen3:/root/.ollama"]
    restart: unless-stopped

  qwen3-coder:
    image: ollama/ollama:latest
    container_name: ollama-qwen3-coder
    volumes: ["./ollama_models/quen3-coder:/root/.ollama"]
    restart: unless-stopped

  model-importer:
    image: ollama/ollama:latest
    entrypoint: /bin/sh
    command: >
      -c "sleep 10;
      OLLAMA_HOST=mistral:11434 ollama pull mistral:latest &&
      OLLAMA_HOST=llama3:11434 ollama pull llama3.1:latest &&
      OLLAMA_HOST=gemma3:11434 ollama pull gemma3:latest &&
      OLLAMA_HOST=deepseek:11434 ollama pull deepseek-r1:latest &&
      OLLAMA_HOST=qwen3:11434 ollama pull qwen3:latest &&
      OLLAMA_HOST=qwen3-coder:11434 ollama pull qwen3-coder:latest"
    depends_on:
      - mistral
      - llama3
      - gemma3
      - deepseek
      - qwen3
      - qwen3-coder

  dozzle:
    image: amir20/dozzle:latest
    container_name: log-viewer
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "8888:8080"
    restart: unless-stopped
